{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Binary Connect\n",
    "A bare-bones instructional implementation of [BinaryConnect(2015) by Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David](https://arxiv.org/pdf/1511.00363). The rest of the post is organised as follows:\n",
    "\n",
    "1. Introduction (contents of the README)\n",
    "2. Objective\n",
    "3. Some Theory\n",
    "4. Forward Pass\n",
    "5. Backward Pass\n",
    "6. Testing\n",
    "7. Adding Binarization\n",
    "8. Testing\n",
    "9. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "A conversation at work spawned a discussion about the difficulties of successfully deploying Deep Neural Networks on embedded and mobile devices. A [very]rough review reveals three broad approaches:\n",
    "- Training shallower/smaller architectures to reduce parameters.\n",
    "- Pruning redundant network connections.\n",
    "- Quantization of weights while preserving accuracy.\n",
    "\n",
    "The focus of this tutorial is the BinaryConnect paper, which falls in the third category. The paper proposes a training procedure to learn binarized weights (+1, -1) as opposed to full precision weights (32bits or 64bits) with minimal loss in precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Objective\n",
    "We will not be replicating the results of the paper. Rather, this will serve as a proof-of-concept for the ...umm, concept. These are the concession that we will be making in the interest of time.\n",
    "- Working with MNIST and only MNIST.\n",
    "- Training a VERY simple model (logistic regression).\n",
    "- Using slow and un-optimised Python.\n",
    "\n",
    "The last point it actually more important that you might think. The true potential of such approaches is unlocked while using specialised hardware and software which actually exploits the single bit weights. But you won't get to see that with Python because I will still use 32 bit integers to represent +1 and -1. So there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some Theory\n",
    "For those unfamiliar with back-propagation, there's a excellent post on the topic by [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html). Here's a <INSERT_FAVORITE_BIG_NUMBER> foot overview using logistic regression model.\n",
    "\n",
    "### 3.1. Model for multi-class logistic regression\n",
    "I define a multi-class classification problems as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "X & \\sim input\\ matrix[batch, features] \\\\\n",
    "W & \\sim weight\\ matrix[features, classes] \\\\\n",
    "\\textbf{b} & \\sim bias\\ vector[1, classes]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The logistic model $f$ is defined as\n",
    "$$\n",
    "f(X, W, \\textbf{b}) = softmax(X.W + \\textbf{b})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "softmax(v_{ij}) = \\frac{e^{v_{ij}}}{\\sum_k e^{v_{ik}}}\n",
    "$$\n",
    "\n",
    "$f$ returns the *logits*, which are the model output, which in this case will be a probability distribution for every sample against every possible class. We can calculate the error from the correct class label using any number of loss/distance measures. In the spirit of needlessly complicating things, let's continue with [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy). Amongst other things, a cross-entropy as an error measure is helpful as it allows weight updates even when the activations are close to saturation, i.e. when the error gradient is very close to 0. Additionally, the combination of a softmax output with cross entropy error gives a very compact and elegant error gradient w.r.t. the input to the softmax, which in this case is $(X.T + \\textbf{b})$.\n",
    "\n",
    "We define cross entropy loss $XE$ for a target distribution $\\textbf{t}$ and predicted distribution $\\textbf{p}$ as  follows\n",
    "$$\n",
    "XE(\\textbf{t}, \\textbf{p}) = - \\sum_i t_i log(p_i)\n",
    "$$\n",
    "where $i$ denotes the class(or dimension) index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Calculating error gradients\n",
    "\n",
    "Training the network requires calculating the error gradients w.r.t the parameters: $W, \\textbf{b}$. And since the output is a function of the parameters, we can just keep applying the chain-rule repeatedly to get the error gradients w.r.t. the parameters, starting from the gradient w.r.t. the output.\n",
    "\n",
    "This repeated and recursive transfer of the error is called *backpropagating the error*. If you're not happy with this definition, raise a PullRequest with a better one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I will go over the notation and indices once again because it gets pretty gnarly. \n",
    "\n",
    "- $X$: uppercase letters represent row-ordered matrices.\n",
    "- $\\textbf{x}$: bold lowercase letters represent vectors. Here $\\textbf{x}$ is a single sample of the batched data in $X$.\n",
    "- $x_{ij}$: lowercase normal font letters represent single values. $x_{ij}$ represents the $j^{th}$ column element for the $i^{th}$ row in the input matrix $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients for logits $l$**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial l_{ij}} &= \\frac{\\partial XE(\\textbf{t}, \\textbf{l}_i)}{\\partial l_{ij}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the gradient is being calculated for a single element $(i, j)$ of the logits matrix $L$. For clarity, I'm assuming there's only one sample in the batch (i.e. $i=1$) which leaves us with logits vector $\\textbf{l}_i$. I have omitted the first(sample or batch) index $i$ henceforth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with that out of the way, let's continue.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial l_j} &= \\frac{\\partial XE(\\textbf{t}, \\textbf{l})}{\\partial l_j} \\\\ \\\\\n",
    "&= \\frac{ \\partial(- \\sum_k t_k log (l_k)) }{\\partial l_j} \\\\ \\\\\n",
    "&= \\frac{ - \\sum_k t_k \\partial log (l_k) }{\\partial l_j}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As a general rule, while introducing any new function or expression that involves indices, make it a point to declare new index placeholders - it will lead to fewer errors and a lot less confusion.\n",
    "\n",
    "\n",
    "\n",
    "Here, $k$ iterates over all class labels. Expanding the sigma,  for $m$ class labels, we will have $m$ different partial derivative terms. And for exactly one term, the numerator and denominator will have the same index. Concretely, for one term, $k = j$ and for all other $n-1$ terms, $k \\neq j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Rightarrow \\frac{\\partial XE}{\\partial l_j} &= - t_{k=j}\\frac{\\partial log(l_{k=j})}{\\partial l_j} -  \\frac{  \\sum_{k \\neq j} t_k \\partial log (l_k) }{\\partial l_j}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be tempting to remove the second term in haste, by observing that the indices do not match and concluding that the derivative w.r.t. $l_j$ will be zero. But, recall the definition of $softmax$, where we divide by the sum of all label probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients for linear transformation $Z$**\n",
    "\n",
    "I'm introducing a new variable $Z$ defined as \n",
    "$$ Z = X.W + \\textbf{b} $$\n",
    "which links to the logits as follows\n",
    "$$ l_{j} = softmax(z_{j}) $$\n",
    "\n",
    "Therefore, to calculate the derivatives for the parameters, we need to calculate the gradient w.r.t. $z_j$ using backpropagation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial z_j} &= \\frac{\\partial XE}{\\partial l_s} \\times  \\frac{\\partial l_s}{\\partial z_j} \\\\ \\\\\n",
    "where\\ \\frac{\\partial l_s}{\\partial z_j} &= \\frac{\\partial softmax(z_s) }{\\partial z_j} \\\\ \\\\\n",
    "and \\ softmax(z_s) &= \\frac{e^{z_s}}{\\sum_v e^{z_v}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of logit vector $l_s$ in terms of the softmax. \n",
    "\n",
    "$$\n",
    "l_s = softmax(z_s) =  \\frac{e^{z_s}}{\\sum_v e^{z_v}}\n",
    "$$\n",
    "\n",
    "While taking the derivative of the denominator in the $softmax$ w.r.t. $z_j$, remeber that $v$ iterates over all label indices and one of them will be equal to $j$. Only for that index, will the deivative exist and equal to $e^{z_j}$; for all other indices, the derivative will be 0. With that, we can compute the gradient w.r.t. $z_j$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l_s}{\\partial z_j} &= \\frac{\\partial q_s }{\\partial z_j} \\\\ \\\\\n",
    "= \\frac{ e^{z_s}  \\sum_v e^{z_v}  - e^{z_s} e^{z_j}}{  (\\sum_v e^{z_v})^2 } \\tag {for s$ = j$} =&  \\frac{ e^{z_s}(  \\sum_v e^{z_v}  - e^{z_j})}{  (\\sum_v e^{z_v})^2 } \\\\ \\\\\n",
    "and\\ \\  \\ & \\frac{ 0  - e^{z_s} e^{z_j}}{  (\\sum_v e^{z_v})^2 } \\tag{for s$ \\neq j$} \\\\ \\\\\n",
    "\\\\\n",
    "For \\ (s=j):&\\  \\frac{ e^{z_s} }{ \\sum_v e^{z_v} } \\bigg(  1 -  \\frac{ e^{z_j} }{ \\sum_v e^{z_v} } \\bigg)  = l_s(1 - l_j) \\\\ \\\\\n",
    "For \\ (s \\neq j):&\\  - \\frac{ e^{z_s} }{ \\sum_v e^{z_v} } \\frac{ e^{z_j} }{ \\sum_v e^{z_v} }   = - l_s  l_j \\\\ \\\\\n",
    "\\end{align} \\\\ \\\\\n",
    "$$\n",
    "\n",
    "I really, *really* tried trying to use those large curly braces you get with `begin{cases}` in Latex, but they just weren't working well with `\\frac` in MathJax. Anyways, now we can compute the error gradients w.r.t. $z_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial z_j} &= \\frac{\\partial XE}{\\partial l_s} \\times  \\frac{\\partial l_s}{\\partial z_j} \\\\ \\\\\n",
    "&= - \\bigg(   t_{k=s}\\frac{\\partial log(l_{k=s})}{\\partial l_s}  +  \\frac{  \\sum_{k \\neq j} t_k \\partial log (l_k) }{\\partial l_s}   \\bigg) \\times \\frac{\\partial l_s}{\\partial z_j} \\\\ \\\\\n",
    "&= - \\bigg(   \\frac{t_{k=s}}{l_{k=s}} \\frac{\\partial l_s}{\\partial z_j}  +  \\sum_{k \\neq j} \\frac{   t_k  }{ l_k }  \\frac{\\partial l_s}{\\partial z_j}   \\bigg)  \\\\ \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "In the first term, index $k = s = j$, and in the second, $k = s \\neq j$. Using the gradients computed before, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "For\\ (k = s = j)&: \\ \\frac{t_{k=s = j}}{l_{k=s = j}} l_{s=j}(1 - l_j) = t_{k= j} (1 - l_j) \\\\ \\\\\n",
    "For\\ (k = s \\neq j)&: \\ \\sum_{k \\neq j} \\frac{   t_k  }{ l_k } \\times (-l_{s=k \\neq j} l_j)  = - \\sum_{k \\neq j} t_k l_j \\\\ \\\\\n",
    "Adding\\ both\\ \\ldots \\\\ \\\\\n",
    "\\frac{\\partial XE}{\\partial z_j} &= - \\bigg(   t_{k = j} (1 - l_j) - \\sum_{k \\neq j} t_k l_j   \\bigg) \\\\ \\\\\n",
    "&= -\\bigg(   t_{k=j} - l_j t_{k=j}    - l_j \\sum_{k \\neq j} t_k   \\bigg) & \\text{since logit $l_j$ does not depend on index $k$ } \\\\ \\\\\n",
    "&= -\\bigg(   t_{k=j} - l_j \\bigg( \\sum_{k \\neq j} t_k  + t_{k=j} \\bigg) \\bigg) \\\\ \\\\\n",
    "&= - (t_{k=j} - l_j) & \\text{$t_k$ is a probability distribution: $\\sum_k t_k = 1$} \\\\ \\\\\n",
    "&= l_j - t_j\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the final error gradient w.r.t. the linear transformation of the inputs and the parameters is the difference between the predicted value and the target value. With that, we get to the final gradient calculations w.r.t. the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
