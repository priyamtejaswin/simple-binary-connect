{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Binary Connect\n",
    "A bare-bones instructional implementation of [BinaryConnect(2015) by Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David](https://arxiv.org/pdf/1511.00363). The rest of the post is organised as follows:\n",
    "\n",
    "1. Introduction (contents of the README)\n",
    "2. Objective\n",
    "3. Some Theory\n",
    "4. Forward Pass\n",
    "5. Backward Pass\n",
    "6. Testing\n",
    "7. Adding Binarization\n",
    "8. Testing\n",
    "9. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "A conversation at work spawned a discussion about the difficulties of successfully deploying Deep Neural Networks on embedded and mobile devices. A [very]rough review reveals three broad approaches:\n",
    "- Training shallower/smaller architectures to reduce parameters.\n",
    "- Pruning redundant network connections.\n",
    "- Quantization of weights while preserving accuracy.\n",
    "\n",
    "The focus of this tutorial is the BinaryConnect paper, which falls in the third category. The paper proposes a training procedure to learn binarized weights (+1, -1) as opposed to full precision weights (32bits or 64bits) with minimal loss in precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Objective\n",
    "We will not be replicating the results of the paper. Rather, this will serve as a proof-of-concept for the ...umm, concept. These are the concession that we will be making in the interest of time.\n",
    "- Working with MNIST and only MNIST.\n",
    "- Training a VERY simple model (logistic regression).\n",
    "- Using slow and un-optimised Python.\n",
    "\n",
    "The last point it actually more important that you might think. The true potential of such approaches is unlocked while using specialised hardware and software which actually exploits the single bit weights. But you won't get to see that with Python because I will still use 32 bit integers to represent +1 and -1. So there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Some Theory\n",
    "For those unfamiliar with back-propagation, there's a excellent post on the topic by [Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html). Here's a <INSERT_FAVORITE_BIG_NUMBER> foot overview using logistic regression model.\n",
    "\n",
    "### 3.1. Model for multi-class logistic regression\n",
    "I define a multi-class classification problems as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "X & \\sim input\\ matrix[batch, features] \\\\\n",
    "W & \\sim weight\\ matrix[features, classes] \\\\\n",
    "\\textbf{b} & \\sim bias\\ vector[1, classes]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The logistic model $f$ is defined as\n",
    "$$\n",
    "f(X, W, \\textbf{b}) = softmax(X.W + \\textbf{b})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "softmax(v_{ij}) = \\frac{e^{v_{ij}}}{\\sum_k e^{v_{ik}}}\n",
    "$$\n",
    "\n",
    "$f$ returns the *logits*, which are the model output, which in this case will be a probability distribution for every sample against every possible class. We can calculate the error from the correct class label using any number of loss/distance measures. In the spirit of needlessly complicating things, let's continue with [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy). Amongst other things, a cross-entropy as an error measure is helpful as it allows weight updates even when the activations are close to saturation, i.e. when the error gradient is very close to 0. Additionally, the combination of a softmax output with cross entropy error gives a very compact and elegant error gradient w.r.t. the input to the softmax, which in this case is $(X.T + \\textbf{b})$.\n",
    "\n",
    "We define cross entropy loss $XE$ for a target distribution $\\textbf{t}$ and predicted distribution $\\textbf{p}$ as  follows\n",
    "$$\n",
    "XE(\\textbf{t}, \\textbf{p}) = - \\sum_i t_i log(p_i)\n",
    "$$\n",
    "where $i$ denotes the class(or dimension) index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Calculating error gradients\n",
    "\n",
    "Training the network requires calculating the error gradients w.r.t the parameters: $W, \\textbf{b}$. And since the output is a function of the parameters, we can just keep applying the chain-rule repeatedly to get the error gradients w.r.t. the parameters, starting from the gradient w.r.t. the output.\n",
    "\n",
    "This repeated and recursive transfer of the error is called *backpropagating the error*. If you're not happy with this definition, raise a PullRequest with a better one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I will go over the notation and indices once again because it gets pretty gnarly. \n",
    "\n",
    "- $X$: uppercase letters represent row-ordered matrices.\n",
    "- $\\textbf{x}$: bold lowercase letters represent vectors. Here $\\textbf{x}$ is a single sample of the batched data in $X$.\n",
    "- $x_{ij}$: lowercase normal font letters represent single values. $x_{ij}$ represents the $j^{th}$ column element for the $i^{th}$ row in the input matrix $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients for logits $l$**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial l_{ij}} &= \\frac{\\partial XE(\\textbf{t}, \\textbf{l}_i)}{\\partial l_{ij}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the gradient is being calculated for a single element $(i, j)$ of the logits matrix $L$. For clarity, I'm assuming there's only one sample in the batch (i.e. $i=1$) which leaves us with logits vector $\\textbf{l}_i$. I have omitted the first(sample or batch) index $i$ henceforth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with that out of the way, let's continue.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial l_j} &= \\frac{\\partial XE(\\textbf{t}, \\textbf{l})}{\\partial l_j} \\\\ \\\\\n",
    "&= \\frac{ \\partial(- \\sum_k t_k log (l_k)) }{\\partial l_j} \\\\ \\\\\n",
    "&= \\frac{ - \\sum_k t_k \\partial log (l_k) }{\\partial l_j}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As a general rule, while introducing any new function or expression that involves indices, make it a point to declare new index placeholders - it will lead to fewer errors and a lot less confusion.\n",
    "\n",
    "\n",
    "\n",
    "Here, $k$ iterates over all class labels. Expanding the sigma,  for $m$ class labels, we will have $m$ different partial derivative terms. And for exactly one term, the numerator and denominator will have the same index. Concretely, for one term, $k = j$ and for all other $n-1$ terms, $k \\neq j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Rightarrow \\frac{\\partial XE}{\\partial l_j} &= - t_{k=j}\\frac{\\partial log(l_{k=j})}{\\partial l_j} -  \\frac{  \\sum_{k \\neq j} t_k \\partial log (l_k) }{\\partial l_j}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be tempting to remove the second term in haste, by observing that the indices do not match and concluding that the derivative w.r.t. $l_j$ will be zero. But, recall the definition of $softmax$, where we divide by the sum of all label probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients for linear transformation $Z$**\n",
    "\n",
    "I'm introducing a new variable $Z$ defined as \n",
    "$$ Z = X.W + \\textbf{b} $$\n",
    "which links to the logits as follows\n",
    "$$ l_{j} = softmax(z_{j}) $$\n",
    "\n",
    "Therefore, to calculate the derivatives for the parameters, we need to calculate the gradient w.r.t. $z_j$ using backpropagation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial z_j} &= \\frac{\\partial XE}{\\partial l_s} \\times  \\frac{\\partial l_s}{\\partial z_j} \\\\ \\\\\n",
    "where\\ \\frac{\\partial l_s}{\\partial z_j} &= \\frac{\\partial softmax(z_s) }{\\partial z_j} \\\\ \\\\\n",
    "and \\ softmax(z_s) &= \\frac{e^{z_s}}{\\sum_v e^{z_v}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of logit vector $l_s$ in terms of the softmax. \n",
    "\n",
    "$$\n",
    "l_s = softmax(z_s) =  \\frac{e^{z_s}}{\\sum_v e^{z_v}}\n",
    "$$\n",
    "\n",
    "While taking the derivative of the denominator in the $softmax$ w.r.t. $z_j$, remeber that $v$ iterates over all label indices and one of them will be equal to $j$. Only for that index, will the deivative exist and equal to $e^{z_j}$; for all other indices, the derivative will be 0. With that, we can compute the gradient w.r.t. $z_j$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l_s}{\\partial z_j} &= \\frac{\\partial q_s }{\\partial z_j} \\\\ \\\\\n",
    "= \\frac{ e^{z_s}  \\sum_v e^{z_v}  - e^{z_s} e^{z_j}}{  (\\sum_v e^{z_v})^2 } \\tag {for s$ = j$} =&  \\frac{ e^{z_s}(  \\sum_v e^{z_v}  - e^{z_j})}{  (\\sum_v e^{z_v})^2 } \\\\ \\\\\n",
    "and\\ \\  \\ & \\frac{ 0  - e^{z_s} e^{z_j}}{  (\\sum_v e^{z_v})^2 } \\tag{for s$ \\neq j$} \\\\ \\\\\n",
    "\\\\\n",
    "For \\ (s=j):&\\  \\frac{ e^{z_s} }{ \\sum_v e^{z_v} } \\bigg(  1 -  \\frac{ e^{z_j} }{ \\sum_v e^{z_v} } \\bigg)  = l_s(1 - l_j) \\\\ \\\\\n",
    "For \\ (s \\neq j):&\\  - \\frac{ e^{z_s} }{ \\sum_v e^{z_v} } \\frac{ e^{z_j} }{ \\sum_v e^{z_v} }   = - l_s  l_j \\\\ \\\\\n",
    "\\end{align} \\\\ \\\\\n",
    "$$\n",
    "\n",
    "I really, *really* tried trying to use those large curly braces you get with `begin{cases}` in Latex, but they just weren't working well with `\\frac` in MathJax. Anyways, now we can compute the error gradients w.r.t. $z_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial z_j} &= \\frac{\\partial XE}{\\partial l_s} \\times  \\frac{\\partial l_s}{\\partial z_j} \\\\ \\\\\n",
    "&= - \\bigg(   t_{k=s}\\frac{\\partial log(l_{k=s})}{\\partial l_s}  +  \\frac{  \\sum_{k \\neq j} t_k \\partial log (l_k) }{\\partial l_s}   \\bigg) \\times \\frac{\\partial l_s}{\\partial z_j} \\\\ \\\\\n",
    "&= - \\bigg(   \\frac{t_{k=s}}{l_{k=s}} \\frac{\\partial l_s}{\\partial z_j}  +  \\sum_{k \\neq j} \\frac{   t_k  }{ l_k }  \\frac{\\partial l_s}{\\partial z_j}   \\bigg)  \\\\ \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "In the first term, index $k = s = j$, and in the second, $k = s \\neq j$. Using the gradients computed before, we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "For\\ (k = s = j)&: \\ \\frac{t_{k=s = j}}{l_{k=s = j}} l_{s=j}(1 - l_j) = t_{k= j} (1 - l_j) \\\\ \\\\\n",
    "For\\ (k = s \\neq j)&: \\ \\sum_{k \\neq j} \\frac{   t_k  }{ l_k } \\times (-l_{s=k \\neq j} l_j)  = - \\sum_{k \\neq j} t_k l_j \\\\ \\\\\n",
    "Adding\\ both\\ \\ldots \\\\ \\\\\n",
    "\\frac{\\partial XE}{\\partial z_j} &= - \\bigg(   t_{k = j} (1 - l_j) - \\sum_{k \\neq j} t_k l_j   \\bigg) \\\\ \\\\\n",
    "&= -\\bigg(   t_{k=j} - l_j t_{k=j}    - l_j \\sum_{k \\neq j} t_k   \\bigg) & \\text{since logit $l_j$ does not depend on index $k$ } \\\\ \\\\\n",
    "&= -\\bigg(   t_{k=j} - l_j \\bigg( \\sum_{k \\neq j} t_k  + t_{k=j} \\bigg) \\bigg) \\\\ \\\\\n",
    "&= - (t_{k=j} - l_j) & \\text{$t_k$ is a probability distribution: $\\sum_k t_k = 1$} \\\\ \\\\\n",
    "&= l_j - t_j\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the final error gradient w.r.t. the linear transformation of the inputs and the parameters is the difference between the predicted value and the target value. With that, we get to the final gradient calculations w.r.t. the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients for the model parameters W, b**\n",
    "\n",
    "$Z$ is a linear transformation of the input $X$, weight $W$ and bias $\\textbf{b}$. Following the dot product, an element $z_j$ can be written as \n",
    "$$\n",
    "z_j = \\sum_i  x_i \\times  w_{ij}\n",
    "$$\n",
    "Thus, it's derivative w.r.t. $w_{ij}$ will be $x_i$. Therefore\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial w_{ij}} &= \\frac{\\partial XE}{\\partial z_j} \\frac{\\partial z_j}{\\partial w_{ij}} \\\\ \\\\\n",
    "&= (l_j - t_j) \\times x_i \\\\ \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "The derivative w.r.t. an element $b_j$ of the bias vector will be 1. Therefore\n",
    "$$\n",
    "\\begin{align} \\\\\n",
    "\\frac{\\partial XE}{\\partial b_{j}} &= \\frac{\\partial XE}{\\partial z_j} \\frac{\\partial z_j}{\\partial b_{j}} \\\\ \\\\\n",
    "&= (l_j - t_j) \\times 1 \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector calculations**\n",
    "\n",
    "The calculations suggest that all operations are performed at an element level. Instead of looping, we can vectorise these operations and calculate the error gradients for the entire matrix - or in this case where the batch size is 1, the entire vector.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial XE}{\\partial Z} &= L - T \\\\ \\\\\n",
    "\\frac{\\partial XE}{\\partial W} &=  X^T. (L - T) \\\\ \\\\\n",
    "\\frac{\\partial XE}{\\partial \\textbf{b}} &=  \\sum_i(l_{ij} - t_{ij}) \\\\ \n",
    "where \\\\\n",
    "L &: \\text{logits matrix of size (BatchSize x NumClasses)} \\\\\n",
    "T &: \\text{target matrix of size (BatchSize x NumClasses)} \\\\\n",
    "Z &: \\text{ $X.W + \\textbf{b}$ (BatchSize x NumClasses)} \\\\\n",
    "X &: \\text{input matrix of size (BatchSize x NumFeatures)} \\\\\n",
    "W &: \\text{weight matrix of size (NumFeatures, NumClasses)} \\\\\n",
    "\\textbf{b} &:  \\text{bias vector of size (1, NumClasses)}\n",
    "\\end{align}\n",
    "$$\n",
    "A way to interpret the last equation is that we are summing the error gradient over all possible samples, for every class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward Pass\n",
    "\n",
    "```python\n",
    "## Imports.\n",
    "import numpy as np\n",
    "np.random.seed(1234) ## For reproducibility.\n",
    "```\n",
    "\n",
    "For the forward pass, we'll use 2 functions. The first to compute the logits `def logistic_model(inputs, weights, bias)`  and the second to compute the cross entropy loss `def ce_loss(output, target)` .\n",
    "\n",
    "```python\n",
    "## For the logits.\n",
    "def logistic_model(inputs, weights, bias):\n",
    "    z = np.dot(x, w) + b\n",
    "    z = z - np.max(z, axis=1, keepdims=True) ## Safe softmax!\n",
    "    _e = np.exp(z)\n",
    "    logits = np.divide(_e, np.sum(_e, axis=1, keepdims=True))\n",
    "    return logits\n",
    "\n",
    "## For the cross entropy.\n",
    "def ce_loss(output, target):\n",
    "    return -1.0 * np.mean( \n",
    "        np.sum(np.multiply(target, np.log(output)), axis=1) \n",
    "    )\n",
    "```\n",
    "\n",
    "There are some deviations from the derivation for practical purposes:\n",
    "- Since softmax involves an exponentiation, the numbers (and the denominator) can explode for even moderately large $z_j$ values. To scale the values down, you subtract the largest value for every row: `z = z - np.max(z, axis=1, keepdims=True) ## Safe softmax!`. This makes no difference to the softmax computation because we take the exponent and then divide by the sum of the exponent.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a &= [1, 2, 3] \\\\ \\\\\n",
    "b &= e^{a},\\ \\ b_1 = e^1 \\\\ \\\\ \n",
    "c_1 &= \\frac{e^1}{(e^1 + e^2 + e^3)} \\\\ \\\\\n",
    "&Multiplying\\ and\\ dividing\\ by\\ e^{-3} \\\\ \\\\ \n",
    "c_1 &= \\frac{e^{-2}}{(e^{-2} + e^{-1} + e^0)} \\\\ \\\\\n",
    "&Thus \\ b,a\\ could\\ be\\ written\\ as \\\\ \\\\\n",
    "b &= [e^{-2} + e^{-1} + e^0] \\\\ \\\\\n",
    "a &= [-2, -1, 0]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- The final loss value after every forward pass will be an average loss over all samples. Hence the `np.mean(...)`  in the cross entropy loss function. Since we are dividing by the batchsize here, we will need to scale the error gradient accordingly. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\frac{\\partial XE}{\\partial Z} = \\frac{1}{n}(L - T) \\\\ \\\\\n",
    "&where\\ \\text{$n$: BatchSize}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
